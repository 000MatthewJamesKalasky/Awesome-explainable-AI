# awesome-explainable-AI
A  collection of research materials on explainable AI
![Open the BlackBox](https://github.com/iversonicter/awesome-explainable-AI/blob/master/fig/blackbox.png)

## Survey
[A survey of methods for explaining black box models](http://arxiv.org/abs/1802.01933)

[A Survey on Explainable Artificial Intelligence (XAI): Towards Medical XAI](http://arxiv.org/abs/1907.07374)

[Explaining Explanations: An Overview of Interpretability of Machine Learning](https://arxiv.org/abs/1806.00069)

[Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)](https://ieeexplore.ieee.org/document/8466590/)

[Explainable Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI](http://arxiv.org/abs/1910.10045)


## Books

[Interpretable Machine Learning A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/)

[Explainable AI: Interpreting, Explaining and Visualizing Deep Learning](http://link.springer.com/10.1007/978-3-030-28954-6)

## Online Videos

## Courses

[Interpretability and Explainability in Machine Learningï¼Œ Harvard University](https://interpretable-ml-class.github.io/)

## Papers

[A unified approach to interpreting model predictions](http://arxiv.org/abs/1705.07874)

[Computationally Efficient Measures of Internal Neuron Importance](http://arxiv.org/abs/1807.09946)

[Axiomatic attribution for deep networks](http://arxiv.org/abs/1703.01365)

[SmoothGrad: removing noise by adding noise](http://arxiv.org/abs/1706.03825)

["Why Should I Trust You?": Explaining the Predictions of Any Classifier](http://arxiv.org/abs/1602.04938)

[Deep inside convolutional networks: Visualising image classification models and saliency maps](http://arxiv.org/abs/1312.6034)

[How important is a neuron?](http://arxiv.org/abs/1805.12233)

[Influence-Directed Explanations for Deep Convolutional Networks](http://arxiv.org/abs/1802.03788)

[Layer-wise relevance propagation for neural networks with local renormalization layers](http://arxiv.org/abs/1604.00825)

[Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)](https://arxiv.org/abs/1711.11279)




## Codes

Captum: [https://github.com/pytorch/captum](https://github.com/pytorch/captum)

SHAP: [https://github.com/slundberg/shap](https://github.com/slundberg/shap)

Lime: [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)

Pytorch-grad-cam: [https://github.com/jacobgil/pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam)

Keras-grad-cam: [https://github.com/jacobgil/keras-grad-cam](https://github.com/jacobgil/keras-grad-cam)

Grad-cam-Tensorflow: [https://github.com/insikk/Grad-CAM-tensorflow](https://github.com/insikk/Grad-CAM-tensorflow)

Deeplift: [https://github.com/kundajelab/deeplift](https://github.com/kundajelab/deeplift)

Eli5: [https://github.com/TeamHG-Memex/eli5](https://github.com/TeamHG-Memex/eli5)

Skater: [https://github.com/oracle/Skater](https://github.com/oracle/Skater)

PDPbox: [https://github.com/SauceCat/PDPbox](https://github.com/SauceCat/PDPbox)

Alibi: [https://github.com/SeldonIO/alibi](https://github.com/SeldonIO/alibi)

TCAV: [https://github.com/tensorflow/tcav](https://github.com/tensorflow/tcav)

PyCEbox: [https://github.com/AustinRochford/PyCEbox](https://github.com/AustinRochford/PyCEbox)




